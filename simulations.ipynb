{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff104d6d",
   "metadata": {},
   "source": [
    "# This file include all the code used to produce simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and define functions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import xgboost\n",
    "import time\n",
    "\n",
    "d = 15\n",
    "theta = np.hstack((np.array([2, 1.5, 1]), np.zeros(d-3)))\n",
    "width = 15\n",
    "\n",
    "def divide0(a, b):\n",
    "    # 0/0 = 0 if the empirical probability is zero\n",
    "    return b and a / b or 0\n",
    "\n",
    "def fpfn(Y_pred, Y_test, G_test):\n",
    "    fpg1 = divide0(np.sum((Y_pred == 1) & (Y_test == 0) & (G_test == 1)), np.sum((Y_test == 0) & (G_test == 1)))\n",
    "    fpg0 = divide0(np.sum((Y_pred == 1) & (Y_test == 0) & (G_test == 0)), np.sum((Y_test == 0) & (G_test == 0)))    \n",
    "    fng1 = divide0(np.sum((Y_pred == 0) & (Y_test == 1) & (G_test == 1)), np.sum((Y_test == 1) & (G_test == 1)))\n",
    "    fng0 = divide0(np.sum((Y_pred == 0) & (Y_test == 1) & (G_test == 0)), np.sum((Y_test == 1) & (G_test == 0)))\n",
    "    return [[fpg0, fng0], [fpg1, fng1]]\n",
    "\n",
    "def index_compute(G, Z, tau):\n",
    "    return G + np.dot(Z, theta) + tau * (np.sum(np.square(Z), axis = 1)/d + 2*Z[:, 0]*np.sum(Z[:, 1:], axis = 1))\n",
    "\n",
    "def classification(tau, rho, n, W, M):   \n",
    "    nt = int(0.7*n) # 70/30 split in traning/test samples\n",
    "    logit = LogisticRegression(penalty = None)\n",
    "    lasso_grid = [{'C': np.logspace(-3, 1, 5)}]\n",
    "    lasso = GridSearchCV(LogisticRegression(penalty = 'l1', solver = \"liblinear\"), lasso_grid, cv=5)\n",
    "    svm_grid = [{'C': np.logspace(-1, 1, 5)}]\n",
    "    svm = GridSearchCV(SVC(), svm_grid, cv=5)\n",
    "    xgb_grid = [{'n_estimators': [50, 100, 150, 200, 250]}]\n",
    "    xgb = GridSearchCV(xgboost.XGBClassifier(use_label_encoder=False, eval_metric = 'error'), xgb_grid, cv=5)\n",
    "    poly = PolynomialFeatures(degree = 3)\n",
    "    \n",
    "    sl = keras.models.Sequential([\n",
    "        keras.Input(shape=[d+1]),\n",
    "        keras.layers.Dense(W, activation='sigmoid'),\n",
    "        keras.layers.Dense(2, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    sl.compile(optimizer='adam', loss='hinge')\n",
    "    dl = keras.models.Sequential([\n",
    "        keras.Input(shape=[d+1]),\n",
    "        keras.layers.Dense(W, activation='relu'),\n",
    "        keras.layers.Dense(W, activation='relu'),\n",
    "        keras.layers.Dense(W, activation='relu'),\n",
    "        keras.layers.Dense(W, activation='relu'),\n",
    "        keras.layers.Dense(W, activation='relu'),\n",
    "        keras.layers.Dense(2, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    dl.compile(optimizer='adam', loss='hinge')\n",
    "\n",
    "    error_l = np.zeros(1)\n",
    "    error_l1 = np.zeros(1)\n",
    "    error_svm = np.zeros(1)\n",
    "    error_xgb = np.zeros(1)\n",
    "    error_sl = np.zeros(1)\n",
    "    error_dl = np.zeros(1)\n",
    "    errors = np.zeros([12, 2])\n",
    "\n",
    "    for m in range(M):\n",
    "        G = np.random.binomial(n=1, p=rho, size=n)\n",
    "        Z = np.random.normal(size = (n, d))\n",
    "        eps = np.random.normal(size = (n, 1))\n",
    "        X = np.c_[G, Z]\n",
    "        index = index_compute(G, Z, tau)\n",
    "        Y = index >= eps[:, 0]\n",
    "        \n",
    "        X_train = X[:nt, :]\n",
    "        X_test = X[nt:, :]\n",
    "        Y_train = Y[:nt]\n",
    "        Y_test = Y[nt:]\n",
    "        G_test = G[nt:]\n",
    "        \n",
    "        logit.fit(X_train, Y_train)        \n",
    "        Y_l = logit.predict(X_test).reshape(-1)\n",
    "        error_l = error_l + np.mean(Y_l != Y_test) / M\n",
    "\n",
    "        X_poly = poly.fit_transform(Z)[:, 1:]\n",
    "        X_train_poly = X_poly[:nt, :]\n",
    "        X_test_poly = X_poly[nt:, :]\n",
    "        lasso.fit(X_train_poly, Y_train)\n",
    "        Y_l1 = lasso.predict(X_test_poly).reshape(-1)\n",
    "        error_l1 = error_l1 + np.mean(Y_l1 != Y_test) / M\n",
    "        \n",
    "        svm.fit(X_train, Y_train)\n",
    "        Y_svm = svm.predict(X_test).reshape(-1)\n",
    "        error_svm = error_svm + np.mean(Y_svm != Y_test) / M\n",
    "\n",
    "        xgb.fit(X_train, Y_train)\n",
    "        Y_xgb = xgb.predict(X_test).reshape(-1)\n",
    "        error_xgb = error_xgb + np.mean(Y_xgb != Y_test) / M\n",
    "        \n",
    "        sl.fit(X_train, Y_train, epochs = 30, verbose = 0)\n",
    "        Y_sl = np.sign(sl.predict(X_test).reshape(-1)) == 1\n",
    "        error_sl = error_sl + np.mean(Y_sl != Y_test) / M\n",
    "        \n",
    "        dl.fit(X_train, Y_train, epochs = 30, verbose = 0)\n",
    "        Y_dl = np.sign(dl.predict(X_test).reshape(-1)) == 1\n",
    "        error_dl = error_dl + np.mean(Y_dl != Y_test) / M\n",
    "\n",
    "        errors = errors + np.concatenate((fpfn(Y_l, Y_test, G_test),\n",
    "                                          fpfn(Y_l1, Y_test, G_test),\n",
    "                                          fpfn(Y_xgb, Y_test, G_test),\n",
    "                                          fpfn(Y_svm, Y_test, G_test),\n",
    "                                          fpfn(Y_sl, Y_test, G_test),\n",
    "                                          fpfn(Y_dl, Y_test, G_test))) / M\n",
    "        \n",
    "    misclass_groups = pd.DataFrame(errors, #/ np.tile([[1-rho, 1-rho], [rho, rho]], (6, 1)) / (n-nt),\n",
    "                               columns=['FP', 'FN'],\n",
    "                               index = ['Logit', '',\n",
    "                                        'LASSO', '',\n",
    "                                        'XGB', '',\n",
    "                                        'SVM', '',\n",
    "                                        'SL', '',\n",
    "                                        'DL', ''])\n",
    "    misclass_groups['Total'] = [np.round(error_l[0], 2), '',\n",
    "                                np.round(error_l1[0], 2), '',\n",
    "                                np.round(error_xgb[0], 2), '',\n",
    "                                np.round(error_svm[0], 2), '',\n",
    "                                np.round(error_sl[0], 2), '',\n",
    "                                np.round(error_dl[0], 2), '']\n",
    "    return misclass_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Monte Carlo Simulation Results: symmetric classification\n",
    "# Sample size n = 1,000\n",
    "np.random.seed(2024)\n",
    "tf.random.set_seed(2024)\n",
    "experiments = 100\n",
    "n = 1000\n",
    "t0 = time.time()\n",
    "tab1 = classification(tau = 1, rho = 0.2, n = n, W = width, M = experiments)\n",
    "tab2 = classification(tau = 1, rho = 0.5, n = n, W = width, M = experiments)\n",
    "tab3 = classification(tau = 0, rho = 0.2, n = n, W = width, M = experiments)\n",
    "tab4 = classification(tau = 0, rho = 0.5, n = n, W = width, M = experiments)\n",
    "group = pd.DataFrame(np.tile([0, 1], 6), tab1.index)\n",
    "tab = pd.concat((group, tab1, tab2, tab3, tab4), axis = 1)\n",
    "print(round(tab, 2))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)\n",
    "\n",
    "print(np.round(tab, 2).to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Monte Carlo Simulation Results: symmetric classification\n",
    "# Sample size n = 10,000\n",
    "np.random.seed(2024)\n",
    "tf.random.set_seed(2024)\n",
    "experiments = 10\n",
    "n = 10000\n",
    "width = 15\n",
    "t0 = time.time()\n",
    "tab1 = classification(tau = 1, rho = 0.2, n = n, W = width, M = experiments)\n",
    "tab2 = classification(tau = 1, rho = 0.5, n = n, W = width, M = experiments)\n",
    "tab3 = classification(tau = 0, rho = 0.2, n = n, W = width, M = experiments)\n",
    "tab4 = classification(tau = 0, rho = 0.5, n = n, W = width, M = experiments)\n",
    "group = pd.DataFrame(np.tile([0, 1], 6), tab1.index)\n",
    "tab = pd.concat((group, tab1, tab2, tab3, tab4), axis = 1)\n",
    "print(round(tab, 2))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)\n",
    "\n",
    "print(np.round(tab, 2).to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa4f6c-5043-46e2-a28e-8fb1b63976f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Monte Carlo Simulation Results: symmetric vs. asymmetric Logit\n",
    "np.random.seed(2024)\n",
    "def logit(phi1, phi0, psi1, psi0, rho, tau, n, M):\n",
    "    nt = int(0.7*n)\n",
    "    logit = LogisticRegression(solver = \"liblinear\")\n",
    "    wlogit = LogisticRegression(solver = \"liblinear\")\n",
    "    ll = np.zeros(M)\n",
    "    wl = np.zeros(M)\n",
    "    theta = np.hstack((np.array([1, 0.9, 0.8]), np.zeros(d-3)))\n",
    "\n",
    "    for m in range(M):\n",
    "        G = np.random.binomial(n=1, p=rho, size=n)\n",
    "        Z = np.random.normal(size = (n, d))\n",
    "        eps = np.random.normal(size = (n, 1))\n",
    "        X = np.c_[G, Z]\n",
    "        index = index_compute(G, Z, tau)\n",
    "        Y = index >= 0.1*eps[:, 0]\n",
    "        \n",
    "        X_train = X[:nt, :]\n",
    "        X_test = X[nt:, :]\n",
    "        Y_train = Y[:nt]\n",
    "        Y_test = Y[nt:]\n",
    "        G_test = G[nt:]\n",
    "\n",
    "        w = (2*Y-1)*((psi1 - phi1)*G + (psi0 - phi0)*(1-G)) + (psi1 + phi1)*G + (psi0 + phi0)*(1-G)\n",
    "        w_train = w[:nt]\n",
    "\n",
    "        logit.fit(X_train, Y_train)\n",
    "        Y_l = logit.predict(X_test).reshape(-1)\n",
    "        wlogit.fit(X_train, Y_train, sample_weight = w_train)        \n",
    "        Y_wl = wlogit.predict(X_test).reshape(-1)\n",
    "        ll[m] = np.mean((psi1 * G_test + psi0 * (1 - G_test)) * ((Y_test == 1) & (Y_l == 0))\n",
    "                             + (phi1 * G_test + phi0 * (1 - G_test)) * ((Y_test == 0) & (Y_l == 1)))\n",
    "        wl[m] = np.mean((psi1 * G_test + psi0 * (1 - G_test)) * ((Y_test == 1) & (Y_wl == 0))\n",
    "                              + (phi1 * G_test + phi0 * (1 - G_test)) * ((Y_test == 0) & (Y_wl == 1)))\n",
    "\n",
    "    pr = np.mean(ll > wl)\n",
    "    summary = pd.DataFrame(ll/wl*(wl>0)).describe()\n",
    "    return pr, summary\n",
    "\n",
    "\n",
    "\n",
    "params = [[1, 1.65, 1, 3, 0.2, 0],\n",
    "            [1, 1.65, 1, 3, 0.5, 0],\n",
    "            [1, 1.65, 1, 3, 0.2, 1],\n",
    "            [1, 2, 1, 3, 0.2, 0],\n",
    "            [1, 1.65, 2, 3, 0.2, 0],\n",
    "            [1, 1.65, 3, 3, 0.2, 0],\n",
    "            [1, 1.65, 1, 4, 0.2, 0],\n",
    "            [1, 1.65, 2, 3, 0.2, 0],\n",
    "            [1, 1.65, 3, 3, 0.2, 0]]\n",
    "\n",
    "pr = np.zeros(len(params))\n",
    "summary = np.zeros((5, len(params)))\n",
    "\n",
    "t0 = time.time()\n",
    "M = 5000\n",
    "# n = 1,000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 1000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr1000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# n = 5,0000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 5000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr5000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# n = 10,0000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 10000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr10000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# Print Table 2\n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.vstack((pr1000,pr5000,pr10000)), index=['1,000', '5,000', '10,000'])))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33e3b1-65a6-4be0-81ba-7b8b2fa43b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Monte Carlo Simulation Results: plug-in vs. asymmetric Logit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "np.random.seed(2024)\n",
    "\n",
    "def logit(phi1, phi0, psi1, psi0, rho, tau, n, M):\n",
    "    nt = int(0.7*n)\n",
    "    logit = LogisticRegression(solver = \"liblinear\")\n",
    "    wlogit = LogisticRegression(solver = \"liblinear\")\n",
    "    ll = np.zeros(M)\n",
    "    wl = np.zeros(M)\n",
    "    pl = np.zeros(M)\n",
    "    theta = np.hstack((np.array([1, 0.9, 0.8]), np.zeros(d-3)))\n",
    "\n",
    "    for m in range(M):\n",
    "        G = np.random.binomial(n=1, p=rho, size=n)\n",
    "        Z = np.random.normal(size = (n, d))\n",
    "        eps = np.random.normal(size = (n, 1))\n",
    "        X = np.c_[G, Z]\n",
    "        index = index_compute(G, Z, tau)\n",
    "        Y = index >= 0.1*eps[:, 0]\n",
    "        \n",
    "        X_train = X[:nt, :]\n",
    "        X_test = X[nt:, :]\n",
    "        Y_train = Y[:nt]\n",
    "        Y_test = Y[nt:]\n",
    "        G_test = G[nt:]\n",
    "\n",
    "        w = (2*Y-1)*((psi1 - phi1)*G + (psi0 - phi0)*(1-G)) + (psi1 + phi1)*G + (psi0 + phi0)*(1-G)\n",
    "        w_train = w[:nt]\n",
    "\n",
    "        logit.fit(X_train, Y_train)\n",
    "        #Y_l = logit.predict(X_test).reshape(-1)\n",
    "        wlogit.fit(X_train, Y_train, sample_weight = w_train)        \n",
    "        Y_wl = wlogit.predict(X_test).reshape(-1)\n",
    "        c = G_test * phi1/(phi1+psi1) + (1-G_test) * phi0/(phi0+psi0)\n",
    "        Y_p = 2*(logit.predict_proba(X_test)[:, 1] > c) - 1\n",
    "\n",
    "        wl[m] = np.mean((psi1 * G_test + psi0 * (1 - G_test)) * ((Y_test == 1) & (Y_wl == 0))\n",
    "                        + (phi1 * G_test + phi0 * (1 - G_test)) * ((Y_test == 0) & (Y_wl == 1)))\n",
    "        pl[m] = np.mean((psi1 * G_test + psi0 * (1 - G_test)) * ((Y_test == 1) & (Y_p == 0))\n",
    "                        + (phi1 * G_test + phi0 * (1 - G_test)) * ((Y_test == 0) & (Y_p == 1)))\n",
    "\n",
    "    pr = np.mean(pl > wl)\n",
    "    summary = pd.DataFrame(pl/wl*(wl>0)).describe()\n",
    "    return pr, summary\n",
    "\n",
    "\n",
    "params = [[1, 1.65, 1, 3, 0.2, 0],\n",
    "            [1, 1.65, 1, 3, 0.5, 0],\n",
    "            [1, 1.65, 1, 3, 0.2, 1],\n",
    "            [1, 2, 1, 3, 0.2, 0],\n",
    "            [1, 1.65, 2, 3, 0.2, 0],\n",
    "            [1, 1.65, 3, 3, 0.2, 0],\n",
    "            [1, 1.65, 1, 4, 0.2, 0],\n",
    "            [1, 1.65, 2, 3, 0.2, 0],\n",
    "            [1, 1.65, 3, 3, 0.2, 0]]\n",
    "\n",
    "pr = np.zeros(len(params))\n",
    "summary = np.zeros((5, len(params)))\n",
    "\n",
    "t0 = time.time()\n",
    "M = 5000\n",
    "# n = 1,000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 1000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr1000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# n = 5,0000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 5000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr5000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# n = 10,0000\n",
    "for i in range(len(params)):\n",
    "               pr[i], stats = logit(phi1=params[i][0], phi0=params[i][1],\n",
    "                                   psi1=params[i][2], psi0=params[i][3], rho=params[i][4],\n",
    "                                   tau = params[i][5], n = 10000, M = M)\n",
    "               summary[:, i] = np.array(stats)[3:].reshape(-1)\n",
    "pr10000 = np.round(pr, 2)         \n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.round(summary, 2))))\n",
    "\n",
    "# Print Table 3\n",
    "print(pd.DataFrame.to_latex(pd.DataFrame(np.vstack((pr1000,pr5000,pr10000)), index=['1,000', '5,000', '10,000'])))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cef9c-fdb5-4576-8e2a-d2a090affe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Asymmetric Binary Choice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import xgboost\n",
    "import time\n",
    "\n",
    "np.random.seed(2024)\n",
    "tf.random.set_seed(2024)\n",
    "\n",
    "def logit(phi1, phi0, psi1, psi0, tau, n, M, erm):\n",
    "    rho = 0.2\n",
    "    nt = int(0.7*n)\n",
    "    logit = LogisticRegression(solver = \"liblinear\")\n",
    "    total = 0\n",
    "    errors = np.zeros([2, 2])\n",
    "    theta = np.hstack((np.array([2, 1.5, 1]), np.zeros(d-3)))\n",
    "\n",
    "    for m in range(M):\n",
    "        G = np.random.binomial(n=1, p=rho, size=n)\n",
    "        Z = np.random.normal(size = (n, d))\n",
    "        eps = np.random.normal(size = (n, 1))\n",
    "        X = np.c_[G, Z]\n",
    "        index = index_compute(G, Z, tau)\n",
    "        Y = index >= eps[:, 0]\n",
    "        \n",
    "        X_train = X[:nt, :]\n",
    "        X_test = X[nt:, :]\n",
    "        Y_train = Y[:nt]\n",
    "        Y_test = Y[nt:]\n",
    "        G_test = G[nt:]\n",
    "\n",
    "        w = (2*Y-1)*((psi1 - phi1)*G + (psi0 - phi0)*(1-G)) + (psi1 + phi1)*G + (psi0 + phi0)*(1-G)\n",
    "        w_train = w[:nt]\n",
    "        \n",
    "        logit.fit(X_train, Y_train, sample_weight = w_train)\n",
    "        if erm == 1:\n",
    "            Y_l = logit.predict(X_test).reshape(-1)\n",
    "        else:\n",
    "            c = G_test * phi1/(phi1+psi1) + (1-G_test) * phi0/(phi0+psi0)\n",
    "            Y_l = logit.predict_proba(X_test)[:, 1] > c\n",
    "            \n",
    "        total = total + np.mean(Y_l != Y_test) / M\n",
    "        errors = errors + fpfn(Y_l, Y_test, G_test)   \n",
    "    return total, errors / M\n",
    "\n",
    "def gb(phi1, phi0, psi1, psi0, tau, n, M, erm):\n",
    "    rho = 0.2\n",
    "    nt = int(0.7*n)\n",
    "    #xgb_grid = [{'n_estimators': [50, 100, 150, 200, 250]}]\n",
    "    #xgb = GridSearchCV(xgboost.XGBClassifier(), xgb_grid, cv=10)\n",
    "    xgb = xgboost.XGBClassifier(n_estimators = 150, eval_metric = 'error')\n",
    "\n",
    "    total = np.zeros(1)\n",
    "    errors = np.zeros([2, 2])\n",
    "    theta = np.hstack((np.array([2, 1, 0.9, 0.8]), np.zeros(d-3)))\n",
    "\n",
    "    for m in range(M):\n",
    "        G = np.random.binomial(n=1, p=rho, size=n)\n",
    "        Z = np.random.normal(size = (n, d))\n",
    "        eps = np.random.normal(size = (n, 1))\n",
    "        X = np.c_[G, Z]\n",
    "        index = index_compute(G, Z, tau)\n",
    "        Y = index >= eps[:, 0]\n",
    "        \n",
    "        X_train = X[:nt, :]\n",
    "        X_test = X[nt:, :]\n",
    "        Y_train = Y[:nt]\n",
    "        Y_test = Y[nt:]\n",
    "        G_test = G[nt:]\n",
    "\n",
    "        w = (2*Y-1)*((psi1 - phi1)*G + (psi0 - phi0)*(1-G)) + (psi1 + phi1)*G + (psi0 + phi0)*(1-G)\n",
    "        w_train = w[:nt]\n",
    "        \n",
    "        xgb.fit(X_train, Y_train, sample_weight = w_train)\n",
    "        if erm == 1:\n",
    "            Y_gb = xgb.predict(X_test).reshape(-1)\n",
    "        else:\n",
    "            c = G_test * phi1/(phi1+psi1) + (1-G_test) * phi0/(phi0+psi0)\n",
    "            Y_gb = xgb.predict_proba(X_test)[:, 1] > c\n",
    "                   \n",
    "        total = total + np.mean(Y_gb != Y_test) / M\n",
    "        errors = errors + fpfn(Y_gb, Y_test, G_test)   \n",
    "    return total, errors / M\n",
    "\n",
    "experiments = 5000\n",
    "sample_size = 1000\n",
    "grid_length = 30\n",
    "errors = np.empty([2, 2, grid_length])\n",
    "total = np.empty(grid_length)\n",
    "\n",
    "# Logit: false positive rates\n",
    "t0 = time.time()\n",
    "grid = np.linspace(1, 1.8, grid_length)\n",
    "for i in range(grid_length):\n",
    "    total[i], errors[:, :, i] = logit(phi1 = grid[i], phi0 = 1,\n",
    "                                   psi1 = 1, psi0 = 1, tau = 0,\n",
    "                                   n = sample_size, M = experiments,\n",
    "                                   erm = 1)\n",
    "    \n",
    "f = plt.figure()   \n",
    "plt.plot(grid, errors[0, 0, :], 'b',\n",
    "         grid, errors[1, 0, :], 'b--',\n",
    "         grid, errors[0, 1, :], 'r',\n",
    "         grid, errors[1, 1, :], 'r--')\n",
    "plt.legend(['FP for G=0', 'FP for G=1', 'FN for G=0', 'FN for G=1'])\n",
    "plt.xlabel(r'$\\varphi_0$')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "f.savefig(\"logit_phi0.pdf\", bbox_inches='tight')\n",
    "\n",
    "# Logit: false negative rates\n",
    "grid = np.linspace(1, 1.8, grid_length)\n",
    "for i in range(grid_length):\n",
    "    total[i], errors[:, :, i] = logit(phi1 = 1, phi0 = 1,\n",
    "                                   psi1 = 1, psi0 = grid[i], tau = 0,\n",
    "                                   n = sample_size, M = experiments,\n",
    "                                   erm = 1)\n",
    "    \n",
    "f = plt.figure()   \n",
    "plt.plot(grid, errors[0, 0, :], 'b',\n",
    "         grid, errors[1, 0, :], 'b--',\n",
    "         grid, errors[0, 1, :], 'r',\n",
    "         grid, errors[1, 1, :], 'r--')\n",
    "plt.legend(['FP for G=0', 'FP for G=1', 'FN for G=0', 'FN for G=1'])\n",
    "plt.xlabel(r'$\\psi_0$')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "f.savefig(\"logit_psi0.pdf\", bbox_inches='tight')\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)\n",
    "\n",
    "# Boosting: false positive rates\n",
    "t0 = time.time()\n",
    "grid = np.linspace(1, 3, grid_length)\n",
    "for i in range(grid_length):\n",
    "    total[i], errors[:, :, i] = gb(phi1 = 1, phi0 = grid[i],\n",
    "                                   psi1 = 1, psi0 = 1, tau = 0,\n",
    "                                   n = sample_size, M = experiments,\n",
    "                                   erm = 1)\n",
    "    \n",
    "f = plt.figure()   \n",
    "plt.plot(grid, errors[0, 0, :], 'b',\n",
    "         grid, errors[1, 0, :], 'b--',\n",
    "         grid, errors[0, 1, :], 'r',\n",
    "         grid, errors[1, 1, :], 'r--')\n",
    "plt.legend(['FP for G=0', 'FP for G=1', 'FN for G=0', 'FN for G=1'])\n",
    "plt.xlabel(r'$\\varphi_0$')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "f.savefig(\"boosting_phi0.pdf\", bbox_inches='tight')\n",
    "\n",
    "# Logit: false negative rates\n",
    "grid = np.linspace(1, 3, grid_length)\n",
    "for i in range(grid_length):\n",
    "    total[i], errors[:, :, i] = gb(phi1 = 1, phi0 = 1,\n",
    "                                   psi1 = grid[i], psi0 = 1, tau = 0,\n",
    "                                   n = sample_size, M = experiments,\n",
    "                                   erm = 1)\n",
    "    \n",
    "f = plt.figure()   \n",
    "plt.plot(grid, errors[0, 0, :], 'b',\n",
    "         grid, errors[1, 0, :], 'b--',\n",
    "         grid, errors[0, 1, :], 'r',\n",
    "         grid, errors[1, 1, :], 'r--')\n",
    "plt.legend(['FP for G=0', 'FP for G=1', 'FN for G=0', 'FN for G=1'])\n",
    "plt.xlabel(r'$\\psi_0$')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "f.savefig(\"boosting_psi0.pdf\", bbox_inches='tight')\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time elapsed: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e673053-17e6-4689-98ad-6c9aaaf7dfea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
